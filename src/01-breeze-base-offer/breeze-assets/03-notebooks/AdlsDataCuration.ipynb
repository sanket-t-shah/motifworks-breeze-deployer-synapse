{
  "name": "AdlsDataCuration",
  "properties": {
    "nbformat": 4,
    "nbformat_minor": 2,
    "sessionProperties": {
      "driverMemory": "28g",
      "driverCores": 4,
      "executorMemory": "28g",
      "executorCores": 4,
      "numExecutors": 2,
      "conf": {
        "spark.dynamicAllocation.enabled": "false",
        "spark.dynamicAllocation.minExecutors": "2",
        "spark.dynamicAllocation.maxExecutors": "2",
        "spark.autotune.trackingId": "681d2178-06f1-4022-bbee-6e097a927ead"
      }
    },
    "metadata": {
      "saveOutput": true,
      "synapse_widget": {
        "version": "0.1",
        "state": {}
      },
      "enableDebugMode": false,
      "kernelspec": {
        "name": "synapse_pyspark",
        "display_name": "Synapse PySpark"
      },
      "language_info": {
        "name": "python"
      },
      "sessionKeepAliveTimeout": 30
    },
    "cells": [
      {
        "cell_type": "code",
        "metadata": {
          "jupyter": {
            "outputs_hidden": false,
            "source_hidden": false
          },
          "nteract": {
            "transient": {
              "deleting": false
            }
          },
          "tags": [
            "parameters"
          ]
        },
        "source": [
          "ObjectName = 'ObjectName'\n",
          "ADFConfigTableListId = 0\n",
          "stgaccount_name = \"stgaccount_name\"\n",
          "container_name = 'container_name'\n",
          "relative_path = \"relative_path\"\n",
          ""
        ],
        "outputs": [],
        "execution_count": 20
      },
      {
        "cell_type": "code",
        "metadata": {},
        "source": [
          "from pyspark.sql import SparkSession\n",
          "from pyspark.sql.types import *\n",
          "stgaccount_name = stgaccount_name\n",
          "container_name = container_name\n",
          "relative_path = relative_path\n",
          "\n",
          "print(stgaccount_name)\n",
          "print(container_name)\n",
          "print(relative_path)\n",
          "\n",
          "\n",
          "read_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (\n",
          "    container_name, stgaccount_name, relative_path)\n",
          "\n",
          "df = spark.read.option(\"header\", True)\\\n",
          "    .option(\"delimiter\", \",\")\\\n",
          "    .csv(read_path)\n",
          "\n",
          "df.show()\n",
          "df.count()\n",
          ""
        ],
        "outputs": [],
        "execution_count": 21
      },
      {
        "cell_type": "code",
        "metadata": {
          "jupyter": {
            "outputs_hidden": false,
            "source_hidden": false
          },
          "nteract": {
            "transient": {
              "deleting": false
            }
          }
        },
        "source": [
          "import pyspark.sql.functions as F\n",
          "\n",
          "\n",
          "def removena(df, InputValue, FieldName):\n",
          "    newdf = df\n",
          "    F_name = FieldName.split(\",\")\n",
          "    InputValues = InputValue.split(\",\")\n",
          "    defaultvalue = InputValues[0]\n",
          "    print(f\"defaultvalue is : {defaultvalue}\")\n",
          "    print(type(defaultvalue))\n",
          "    for i in F_name:\n",
          "        colName = i\n",
          "        newdf = newdf.na.fill(value=defaultvalue, subset=[colName])\n",
          "\n",
          "    return newdf\n",
          "\n",
          "\n",
          "def droppingduplicates(df, FieldName):\n",
          "    F_name = FieldName.split(\",\")\n",
          "    df3 = df.dropDuplicates(subset=F_name)\n",
          "\n",
          "    return df3\n",
          "\n",
          "\n",
          "def uppercol(df3, FieldName):\n",
          "    df4 = df3\n",
          "    F_name = FieldName.split(\",\")\n",
          "    for i in F_name:\n",
          "        colName = i\n",
          "        df4 = df4.withColumn(i, F.upper(i))\n",
          "\n",
          "    return df4\n",
          "\n",
          "\n",
          "def replacechars(df4, columns, InputValue):\n",
          "    df5 = df4\n",
          "    from pyspark.sql.functions import regexp_replace\n",
          "    F_name = FieldName.split(\",\")\n",
          "    Chars_removed = InputValue.split(',')\n",
          "    charsremove = \"\".join(Chars_removed)\n",
          "    for i in F_name:\n",
          "        colName = i\n",
          "        df5 = df5.withColumn(i, regexp_replace(i, \"[\"+charsremove+\"]\", \"\"))\n",
          "\n",
          "    return df5\n",
          "\n",
          "\n",
          "def dateformat(df5, FieldName):\n",
          "    df6 = df5\n",
          "    df6 = df5.withColumn(FieldName, F.regexp_replace(\n",
          "        df6[FieldName], \"(\\\\d{2})(\\\\d{2})(\\\\d+)\", \"$1-$2-$3\"))\n",
          "    return df6\n",
          "\n",
          "\n",
          "def lowercolumns(df6, FieldName):\n",
          "    df7 = df6\n",
          "    F_name = FieldName.split(\",\")\n",
          "    for i in F_name:\n",
          "        colName = i\n",
          "        df7 = df7.withColumn(i, F.lower(i))\n",
          "\n",
          "    return df7\n",
          "\n",
          "\n",
          "def titlecase(df7, FieldName):\n",
          "    df8 = df7\n",
          "    F_name = FieldName.split(\",\")\n",
          "    for i in F_name:\n",
          "        colName = i\n",
          "        df8 = df8.withColumn(i, initcap(col(i)))\n",
          "\n",
          "    return df8\n",
          ""
        ],
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "metadata": {
          "jupyter": {
            "outputs_hidden": false,
            "source_hidden": false
          },
          "nteract": {
            "transient": {
              "deleting": false
            }
          }
        },
        "source": [
          "from pyspark import SparkConf\n",
          "from pyspark.sql import SparkSession\n",
          "from notebookutils import mssparkutils\n",
          "\n",
          "import pandas.io.sql as psql\n",
          "import pandas as pd\n",
          "import sys\n",
          "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
          "import pyodbc\n",
          "\n",
          "from pyspark.sql.functions import *\n",
          "from pyspark.sql.types import *\n",
          "import json\n",
          "import datetime\n",
          ""
        ],
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "metadata": {
          "jupyter": {
            "outputs_hidden": false,
            "source_hidden": false
          },
          "nteract": {
            "transient": {
              "deleting": false
            }
          }
        },
        "source": [
          "\n",
          "configserver = '{{SYNPAPSE_WORKSPACE_NAME}}.database.windows.net'\n",
          "configdatabase = 'syndpbreeze'\n",
          "configusername = '{{BREEZE_ADMIN_USERNAME}}'\n",
          "configpassword = '{{BREEZE_ADMIN_PASSWORD}}'\n",
          "\n",
          "\n",
          "conn = pyodbc.connect('DRIVER={ODBC Driver 18 for SQL Server};SERVER='+configserver +\n",
          "                      '; database='+configdatabase+';UID='+configusername+';PWD='+configpassword)\n",
          ""
        ],
        "outputs": [],
        "execution_count": null
      },
      {
        "cell_type": "code",
        "metadata": {
          "jupyter": {
            "outputs_hidden": false,
            "source_hidden": false
          },
          "nteract": {
            "transient": {
              "deleting": false
            }
          },
          "collapsed": false
        },
        "source": [
          "from pyspark.sql.functions import col\n",
          "container_name = container_name\n",
          "sql_query = (\"\"\"Select Distinct ObjectName,FieldName,TransformName,InputValue from [breeze].[BreezeTransformConfig] where\n",
          "ObjectName= {inputvar1} and ADFConfigTableListId = {inputvar2}\"\"\".format(inputvar1='ObjectName', inputvar2=ADFConfigTableListId))\n",
          "\n",
          "Trac_df = pd.read_sql(sql_query, conn)\n",
          "Trac_df = spark.createDataFrame(Trac_df)\n",
          "\n",
          "termObject = Trac_df.collect()\n",
          "\n",
          "for row in termObject:\n",
          "\n",
          "    ObjectName = row['ObjectName']\n",
          "    FieldName = row['FieldName']\n",
          "    TransformName = row['TransformName']\n",
          "    InputValue = row['InputValue']\n",
          "    InputValue = str(InputValue)\n",
          "\n",
          "    print(row)\n",
          "\n",
          "    if(TransformName == 'RemoveDuplicates'):\n",
          "        df = droppingduplicates(df, FieldName)\n",
          "        print(type(FieldName))\n",
          "        print(\"Successfully dropped duplicates records\")\n",
          "        df.show()\n",
          "\n",
          "    if(TransformName == 'ModifytoUpper'):\n",
          "        df = uppercol(df, FieldName)\n",
          "        print(\"Successfully modified to Upper case\")\n",
          "\n",
          "    if(TransformName == 'RemoveSpecialCharacter'):\n",
          "        columns = list(df.columns)\n",
          "        df = replacechars(df, column, InputValue)\n",
          "        print(\"Successfully removed special characters\")\n",
          "\n",
          "    if(TransformName == 'DateFormatting'):\n",
          "        df = dateformat(df, FieldName)\n",
          "        print(\"Successfully converted date formatting\")\n",
          "\n",
          "    if(TransformName == 'ModifytoLower'):\n",
          "        df = lowercolumns(df, FieldName)\n",
          "        print(\"Successfully modified to lower case\")\n",
          "\n",
          "    if(TransformName == 'ModifytoTitleCase'):\n",
          "        df = titlecase(df, FieldName)\n",
          "        print(\"Successfully modified to Title case\")\n",
          "\n",
          "    if(TransformName == 'BlankSpaceRemoval'):\n",
          "        df = removena(df, InputValue, FieldName)\n",
          "        print(\"Successfully dropped null records\")\n",
          "\n",
          "\n",
          "df.show()\n",
          "F_name = FieldName.split(\",\")\n",
          "\n",
          "for i in F_name:\n",
          "    df = df.filter(col(i) != \"\")\n",
          "\n",
          "print(\"Dropping null rows\")\n",
          "df.show()\n",
          "\n",
          "\n",
          "def writeCSVFile(OutputDf, tempDir):\n",
          "    df_2 = OutputDf.coalesce(1)\\\n",
          "        .write\\\n",
          "        .mode(\"overwrite\")\\\n",
          "        .option(\"header\", True)\\\n",
          "        .option(\"delimiter\", \",\")\\\n",
          "        .format(\"csv\")\\\n",
          "        .save(tempDir)\n",
          "\n",
          "\n",
          "def checkPathExists(path):\n",
          "    try:\n",
          "        filelist = mssparkutils.fs.ls(path)\n",
          "        if len(filelist) > 0:\n",
          "            return True\n",
          "        return True\n",
          "    except:\n",
          "        print(\"Exception\")\n",
          "        return False\n",
          "\n",
          "\n",
          "def moveFile(source, destination, sourceFileName, destinationFileName):\n",
          "    if checkPathExists(destination) == False:\n",
          "        mssparkutils.fs.mkdirs(destination)\n",
          "    filelist = mssparkutils.fs.ls(source)\n",
          "    readFileName = \"\"\n",
          "    for file in filelist:\n",
          "        readFileName = file.name\n",
          "\n",
          "    mssparkutils.fs.mv(source+readFileName, destination +\n",
          "                       destinationFileName, overwrite=True)\n",
          "\n",
          "\n",
          "tmpDir = 'abfss://%s@%s.dfs.core.windows.net/Temp/' % (\n",
          "    container_name, stgaccount_name)\n",
          "FilePath = \"abfss://breezegold@\"+stgaccount_name+\".dfs.core.windows.net/Gold/\"\n",
          "print(\"printing tmp dir\")\n",
          "print(tmpDir)\n",
          "print(f\"Container name is :{container_name}\")\n",
          "writeCSVFile(df, tmpDir)\n",
          "\n",
          "moveFile(tmpDir, FilePath+ObjectName+'/', 'part-00000', ObjectName+'.csv')\n",
          "print(\n",
          "    f\"Storage account used for this pipeline execution is : {stgaccount_name}\")\n",
          "print(\"************File Exported Succesfully************\")\n",
          ""
        ],
        "outputs": [],
        "execution_count": null
      }
    ]
  }
}